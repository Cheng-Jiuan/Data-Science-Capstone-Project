# Data-Science-Capstone-Project
應數系旁聽課程筆記-10/5

## 統計學習基礎概念
透過數據建構與機率統計模型對資料進行預測及後續分析

### 統計學習可分為：
- 監督式學習：有目標預測變數
  - 在回歸中，𝑌 是固定可預測的，例如：價格、血壓
  - 在分類問題中，𝑌 在一個有限的的集合中，例如：存活/死亡，數字0-9，癌症類別
  - 我們有訓練數據𝑥 , 𝑦 , ... , (𝑥 , 𝑦 )，這些是測量的觀察值，例如：實例
- 非監督學習：無目標預測變數
  - 沒有結果變量，只有在一組樣本上測量的一組預測因子（特徵）
  - 目標比較模糊，因此需找到行為相似的樣本組、特徵，及找到變化最大的特徵的線性組合
- 半監督學習：
  - 只有對於𝑚的觀察值（𝑚 < 𝑛）才有反應

### 為何要估計𝑓？
- y=𝑓(x)通常被視為一個黑箱，會有可減少和不可減少的誤差
   - 可減少誤差可通過使用最合適的統計學習技術來估計𝑓
   - 不可減少誤差可能包含對預測𝑌有用的未測量的變量，由於我們沒有測量它，所以𝑓不能用它們來預測
- 多數情況下我們有x，但卻不容易知道y，因此我們需使用𝑓

### 如何估計𝑓？
1. 選擇一個模型𝑓𝜃
  - 母體(Parametric)
     - 明確假設
     - 估計一組固定的參數
  - 非母體(Non-parametric)
     - 無明顯假設
     - 需要大量的觀測數據
2. 選擇一個好的衡量（目標函數）進行搭配(fit)
 - 平均平方誤差（最大貌似Maximum likelihood...)
3. 優化（搭配(fit)）以選擇最佳的𝜃 

### 回歸函數𝑓(𝑥)
就平均預測誤差而言，𝑌 的理想或最佳預測：𝑓(𝑥) = 𝐸(𝑌|𝑋 =𝑥) 是在所有函數𝑓上，在所有點𝑋 = 𝑥，使𝐸[（𝑌-𝑓（𝑋））2|𝑋 = ］最小。

### 維度
- 對於小的𝑝即𝑝≤4和大的𝑛來說，近鄰平均法(Nearest neighbor averaging)可以說是相當不錯的
- 當𝑝較大時，近鄰平均法(Nearest neighbor averaging)可能很糟糕。
- 原因是：維度的詛咒
  - 在高維度上，最近的鄰居往往離得很遠。需要從𝑛值中獲取合理的部分𝑦𝑖作為平均值，以降低方差-比如10%。在高維度上，10%的鄰域不再需要是局部的，所以失去了通過局部平均來估計𝐸(𝑌|𝑋 = 𝑥)的精神。
<img width="621" alt="截圖 2022-04-12 下午12 26 52" src="https://user-images.githubusercontent.com/77944202/162880511-d4c28e22-c707-4262-a942-1ada7bb4ce76.png">


